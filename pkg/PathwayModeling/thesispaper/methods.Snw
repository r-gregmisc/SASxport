\section{Methods}

%% \begin{verbatim}
%% $Id$
%% \end{verbatim}

  \subsection{The Pathway}
  
  We examined pathways containing 3, 5, 7, and 9 reactions with both
  linear and branched structures using two sets of parameter values.
  As essentially identical modeling performance was obtained for each,
  we present only the results for a 5-reaction sequence, sampled at
  12, 16, or 25 time points with 3 replicates at each time point. The
  units for time and concentration are arbitrary; for the purpose of
  illustration we will use minutes as the unit of time and
  micromoles/liter ($\mu$M) as the unit of concentration.

<<data, echo=F, eval=F>>=1
# gillespie.out - output from the Gillespie simulation
source('R/getPaperData.R')
source('R/getPaperRates.R')
# trim the Gillespie output
rawdata <- read.table('data/gillespie.out',header=T)[2528:7160,]
# sampling times:
times16 <- c(16:19,20.5,21,seq(22,40,2))
# generating the MCMC input file input16.dat
data <- getPaperData(rawdata,20,times16,npoints=3, varVector=c(50,50,50,50,50))
rates <- getPaperRates(data,1:12,13:48)
temp <- matrix(0,nrow=16,ncol=11)
for (i in 1:16) temp[i,] <- (rates[3*i,]+rates[3*i-1,]+rates[3*i-2,])/3
colnames(temp) <- colnames(rates)
write.table(temp,file="data/input16.dat",row.names=F,quote=F)
@  
    The reaction sequence is
    \begin{reaction} source \yields R1 \yields R2 \yields
      R3 \yields R4 \yields R5 \yields sink \end{reaction}
    and the equations that were used in the Gillespie simulation
    are
    \begin{chemarray*}
      source &\yields^{k_1}& R1\\
      R1 + E1 &\eqbm^{k_2}_{k_3}& R1E1 \yields^{k_4} R2 + E1\\
      R2 + E2 &\eqbm^{k_5}_{k_6}& R2E2 \yields^{k_7} R3 + E2\\
      R3 + E3 &\eqbm^{k_8}_{k_9}& R3E3 \yields^{k_{10}} R4 + E3\\
      R4 + E4 &\eqbm^{k_{11}}_{k_{12}}& R4E4 \yields^{k_{13}} R5 + E4\\
      R5 &\yields^{k_{14}}& sink
    \end{chemarray*}

  \subsection{The Experiment}

    To estimate the kinetic parameters of the enzymes in a sequence of
    reactions, we need to find expressions of the form
    \[
    \text{reaction velocity} = f(\mathit{[substrate],[enzyme]})
    \]
    These expressions are found by perturbing a reaction network
    and observing the metabolite concentrations as the system relaxes
    back to a steady state. Specifically, the Gillespie simulation was 
    run with the initial values for the reactant concentrations until 
    a steady state was achieved. The steady state appeared to have 
    been reached by \textit{time} = 15 min. At \textit{time} = 20 min
    the concentration of $R1$ was increased to 10000 $\mu$M. At the indicated time points the metabolite concentrations are
    measured, and each reaction velocity is calculated as the
    slope of the curve of metabolite concentration as a function of
    time.

    \subsection{The data}

    For the 5-reaction sequence of reactions 
    the perturbation of $R1$ at $time = 20$ results in the time
    courses plotted in Figure~\ref{pulse}.
    For each time point there are 5 values for the reactant
    concentrations and 5 values for the estimated reaction rates.
    The data at each time point was generated by sampling 3 values for
    each reactant from a normal distribution centered on the Gillespie
    output and with a standard deviation of 50 $\mu$M, i.e., the 
    measurement error is 50 $\mu$M. Three sets of time
    points containing 12,16, and 25 points were used.
    The reaction velocities were estimated with the
    \textit{smooth.spline()} function in \textit{R}~\cite{R}.

    Note that we are dealing with two different types of distributions.
    One is the distribution of data values that arises from
    experimental error. The other is the distribution of model
    parameter values generated by MCMC simulation. Uncertainty in the
    parameter values is reduced by increasing the number of data
    points within the fixed time interval (Figure~\ref{converged}) and
    by decreasing experimental error.  Unfortunately, there is, 
    in general, no closed form solution for these relationships.

  \subsection{The Biochemical Model}

    Individual reactions were fit using the Michaelis-Menten
    equation \cite{Michaelis13} for individual enzymes. This is a
    reaction of the form
    \begin{reaction}\label{eqnForm}
      E + S \eqbm^{k_1}_{k_2} ES \yields^{k_3} E + P
    \end{reaction}
    where
    \begin{eqnarray*}
      S &=& \text{substrate concentration}\\
      E &=& \text{free enzyme concentration}\\
      ES &=& \text{concentration of the enzyme-substrate complex}\\
      P &=& \text{product concentration}
    \end{eqnarray*}
    It has been shown \cite{Briggs25} that in a steady state the rate
    of the reaction $v$ is
    \begin{equation}
    v = \frac{V_{max}S}{K_m+S} 
    \end{equation}
    where
    \begin{eqnarray*}
      V_{max} &=& (E+ES)k_3 = \text{maximum reaction velocity}\\[2mm]
      K_m &=& \frac{k_2+k_3}{k_1} = \text{substrate concentration at
      half-maximal velocity}
    \end{eqnarray*}
    This form of the equation is very useful because $v$ and $S$ are
    usually measurable and $V_{max}$ and $K_m$ can be obtained by
    fitting  equation 3 to the data. In contrast, modeling $v$ as a
    function of the individual rate constants is less useful because
    that requires the measurement of the concentration of the
    enzyme-substrate complex which is technically difficult.

    In this application we are dealing with sequences of reactions
    which are not in a steady state, so we cannot use the
    Michaelis-Menten equation directly. Instead, we use equations
    of the form
    \begin{equation}
    v = \frac{aS}{b+S} - \frac{cP}{d+P} 
    \end{equation}
    
    The coefficients $a$, $b$, $c$, and $d$ in the equations can be 
    estimated with data
    obtained following a change in the concentration of one
    of the reactants. For example, four equations
    can be fit to the data plotted in Figure~\ref{pulse}:
    \begin{align*}
      \deriv{R2}{t} &= v_2 = \frac{d_1R1}{d_2 + R1} -
      \frac{d_3R2}{d_4 + R2}\\[5mm]
      \deriv{R3}{t} &= v_3 = \frac{d_3R2}{d_4 + R2} - \frac{d_5R3}{d_6
      + R3}\\[5mm]
      \deriv{R4}{t} &= v_4 = \frac{d_5R3}{d_6 + R3} - \frac{d_7R4}{d_8
      + R4}\\[5mm]
      \deriv{R5}{t} &= v_5 = \frac{d_7R4}{d_8 + R4} - d_9R5
    \end{align*}

    \SweaveInput{figure2}

  \subsection{Statistical Models}

    The statistical model of the parameters is the product of a prior
    distribution and a likelihood distribution, where the prior
    expresses our current understanding of the parameter values (via
    probability distributions) and the likelihood is our model for the
    probability of the parameters given the data.
  
    The distributions of the parameters to be estimated (rate
    constants, $d_i$; and the common standard deviation of the
    observed measurements $\sigma$), are necessarily non-negative
    and have maximum values that are bounded by the physical nature of
    the system. Consequently, we model each of these parameters using
    scaled $\chi^2$ distributions with $5$ degrees of freedom:
    \[
    \frac{3d_i}{d_i^{*}} \sim \chi_5^2
    \]
    and
    \[
    \frac{3\sigma}{\sigma^{*}} \sim \chi_5^2
    \]
    where $\{d_i^{*}\}$ and $\sigma^*$ are estimates of the
    corresponding parameters before any data are collected.  These
    distributions are strictly positive, place the modes of each $d_i$
    at $d_i^{*}$ ($\sigma$ at $\sigma^*$), and have standard
    deviations of $3\sqrt{10} ~ d_i^{*} \approx 9.5 ~ d_i^{*}$ ( $9.5
    \sigma^*$).  For our modeling, we selected $d_i^{*} \equiv 50$
    $\mu$M and $\sigma_i^{*} \equiv 2$ $\mu$M (see figure \ref{priorPlot}).

    \SweaveInput{priorPlot}
  
    The likelihood is the probability of the estimated reaction
    velocities $v_j$ given the model.  The error in the velocity
    estimates is assumed to be $N(0,\sigma^2)$ so that the reaction
    velocities $v_j$ have a normal distribution:
    \[
    v_j \sim N(\psi_j,\sigma^2)
    \]
    where
    \[
    \psi_j = \frac{aS_j}{b+S_j} - \frac{cP_j}{d+P_j} 
    \]
    The observed data consist of values for $v_j$, $S_j$, and $P_j$. For
    example, for the 5-reaction model we have
    \begin{align*}
      v_2 &\sim N\left(\frac{d_1R1}{d_2+R1} -
      \frac{d_3R2}{d_4+R2}, \;\; \sigma^2\right)\\
      v_3 &\sim N\left(\frac{d_3R2}{d_4+R2} -
      \frac{d_5R3}{d_6+R3}, \;\; \sigma^2\right)\\
      v_4 &\sim N\left(\frac{d_5R3}{d_6+R3} -
      \frac{d_7R4}{d_8+R4}, \;\; \sigma^2\right)\\
      \intertext{and}
      v_5 &\sim N\left(\frac{d_7R4}{d_8+R4} - d_9R5, \;\; \sigma^2\right)
    \end{align*}
    In this case there are 10 parameters to be estimated: the 9
    coefficients $d_1$ -- $d_9$ and $\sigma^2$.

    \subsubsection{MCMC sampling algorithms}

    The efficiency of a MCMC simulation \cite{Metropolis53,Hastings70}
    depends heavily on the method used to find candidate points to add
    to the Markov chain [$q(\cdot |\cdot)$].  For this reason,
    different methods have been developed to increase the efficiency
    of sampling. We have used three algorithms
    from the Hydra library \cite{Hydra}: component-wise Metropolis,
    all-components Metropolis, and Normal Kernel Coupler
    \cite{Warnes00}.

    The component-wise Metropolis and the all-components Metropolis
    algorithms both operate on single Markov chains. The
    component-wise algorithm (Figure~\ref{1comp}) generates candidate
    points by changing the value of only one component (dimension) of
    the current state at each iteration by sampling from a univariate
    normal distribution centered at the current point.

    \SweaveInput{figure3}

    The all-components algorithm (Figure~\ref{allComp}) changes all
    the components simultaneously by sampling from a multivariate
    normal distribution centered at the current point.  The
    component-wise Metropolis algorithm has the advantage of
    simplicity but may move very slowly if the components are highly
    correlated. The all-components Metropolis avoids the problems with
    correlation, if an appropriate covariance matrix is supplied, by
    updating all components simultaneously but may perform poorly in
    high dimensions. A problem with both of these algorithms is that
    if the component distributions have two or more modes separated by
    areas of low probability, the simulator can get stuck in the
    vicinity of one mode and fail to visit the other. The NKC
    algorithm is designed to avoid this difficulty.
    
    The Normal Kernel Coupler (NKC) algorithm operates on multiple
    chains, so that there are several ``current'' states. The NKC uses
    a normal kernel density estimate based on the entire set of
    current states as the proposal distribution for choosing candidate
    states. Since the algorithm uses the entire set of current points
    it can move over areas of low probability in the parameter space,
    especially if the user takes care to seed the starting set with a
    few points in each mode.

 \subsection{Computation}

  \subsubsection{Tuning}

  All three MCMC algorithms require an appropriate covariance matrix
  to move efficiently through the parameter space. The optimal
  covariance matrix for each method is a scaled version of the true
  covariance matrix. To obtain this covariance matrix, we performed
  several runs of the all-components algorithm. After each run, two
  calculations were performed.  First, an estimate of the posterior
  covariance matrix was computed from the MCMC output to be used in
  the next run.  Second, \textit{mcgibbsit} (see \ref{convergence}
  below) was used to estimate the necessary length of the next
  run. Only three runs were required to achieve both an acceptable
  estimate according to \textit{mcgibbsit} and a covariance matrix
  with good performance for all three algorithms.

  \subsubsection{Execution}

  To enable a ``fair'' comparison between the methods, each MCMC
  algorithm was started at the same starting point (jittered to
  provide the multiple starting states of the NKC) and the proposal
  covariance was set to an appropriately scaled version of the
  posterior covariance matrix described above. With this covariance
  matrix, each of the algorithms demonstrated an appropriate
  acceptance rate.

  \subsubsection{Convergence}
  \label{convergence}

  The MCMC simulations were run until the Markov chains have reached
  stable distributions as assessed by the \textit{mcgibbsit}
  algorithm \cite{Warnes00}. \textit{mcgibbsit} calculates the
  number of iterations necessary to estimate a user-specified quantile
  $q$ to within $\pm r$ with probability $s$, i.e.,
  \textit{mcgibbsit} indicates when the MCMC sampler has run long
  enough to provide good credible interval estimates. The defaults,
  which are used in this paper, are $q=0.025$, $r=0.0125$, and
  $s=0.95$. These values generate estimates of the 2.5\% and 97.5\%
  quantiles to $\pm$ 0.0125 quantiles with probability 0.95.

  \subsubsection{Burn-in and Thinning}

  Many MCMC papers recommend the removal of an initial ``burn-in''
  segment of the MCMC run since these initial MCMC samples can
  be heavily influenced by the initial starting point, and can result
  in bias in the computed estimates.  We adopted an alternative
  approach to avoiding bias in our estimates; each MCMC sampler was
  run until \textit{mcgibbsit} was satisfied that the quantile
  estimates were properly estimated.  One necessary condition for
  \textit{mcgibbsit} to declare that these quantiles are properly
  estimated is stability of these estimates.  This will only occur
  once any extreme initial points in the MCMC chain have been
  overwhelmed by later points more representative of the true
  posterior distribution.

  Likewise, the high serial correlation of most MCMC samples has
  induced many MCMC papers to recommend \emph{thinning} the output of
  the MCMC sampler by discarding all but 1 out of,say, every 10 or 100
  generated values. While this does not improve in the quality of
  estimates obtained from the MCMC run itself, it can dramatically
  reduce both storage requirements and run time, since writing results
  to disk can require considerably more time than computing the points
  themselves. In our simulations, we only thinned the component-wise
  Metropolis (by a factor of 250), to reduce the storage requirements
  for its significantly longer run length.



  